{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c0f70214c0dd213f07f54ee5d6e0ea644bdbba35113c9bfe8aaa0d1db03ad5dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train.csv\")\n",
    "df=data.drop(columns=[\"id\"])\n",
    "\n",
    "sample = np.random.choice(data.index, size=20000)\n",
    "df = df.iloc[sample]\n",
    "\n",
    "y=df['target']\n",
    "y=y.values.reshape(20000,1)\n",
    "# x=sample_df.drop('target', axis=1)\n",
    "for j in range(20000):\n",
    "    i=y[j]\n",
    "    if i=='Class_1':\n",
    "        y[j]=0\n",
    "    if i=='Class_2':\n",
    "        y[j]=1\n",
    "    if i=='Class_3':\n",
    "        y[j]=2\n",
    "    if i=='Class_4':\n",
    "        y[j]=3\n",
    "    if i=='Class_5':\n",
    "        y[j]=4\n",
    "    if i=='Class_6':\n",
    "        y[j]=5\n",
    "    if i=='Class_7':\n",
    "        y[j]=6\n",
    "    if i=='Class_8':\n",
    "        y[j]=7\n",
    "    if i=='Class_9':\n",
    "        y[j]=8\n",
    "\n",
    "df=df.astype(float)\n",
    "x=np.array(df.iloc[:,0:75])\n",
    "x=np.divide(x,np.max(x,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(np.divide(1,(1+np.exp(-x))))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# Output (prediction) formula\n",
    "def output_formula(x, weights, bias):\n",
    "    return(sigmoid(np.dot(x,weights)+bias))\n",
    "\n",
    "# Error (log-loss) formula\n",
    "def error_formula(y, output):\n",
    "    return(-(np.dot(y.T,np.log(output))+np.dot((1-y).T,np.log(1-output))))\n",
    "\n",
    "def error_term_formula(x, y, output):\n",
    "    return (y-output)*np.dot(sigmoid_prime(x),x.T)\n",
    "\n",
    "# Gradient descent step\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    weights=weights + learnrate*((y-output_formula(x,weights,bias))*x)\n",
    "    bias=bias+learnrate*(y-output_formula(x,weights,bias))\n",
    "    return weights,bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "========== Epoch 0 ==========> Train loss:  9338.982043692173\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  5978.095122298573\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  5950.863515555184\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  5950.8441032264545\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  5950.8440626771735\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  5950.844062591923\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  5950.844062591743\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  5950.844062591743\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  5950.844062591743\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  5950.844062591743\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  13806.798416047977\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  7553.919667040904\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  7553.918553493406\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  7553.918553493405\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  11346.938184479332\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  6717.563915302725\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  6716.37826711862\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  6716.378231846429\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  6716.378231845071\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  6716.378231845072\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  6716.378231845072\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  6716.378231845072\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  6716.378231845072\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  6716.378231845072\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  5128.361751821642\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  4318.745547983688\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  4123.709903881924\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  4107.682002888827\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  4106.602014260485\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  4106.480839723673\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  4106.463501651897\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  4106.460897288784\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  4106.460503049004\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  4106.460443299784\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  4632.504302643654\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  4000.593573420895\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  3745.105990385761\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  3693.2409312471755\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  3686.6778681131345\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  3685.7682383815713\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  3685.596398867605\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  3685.5580691393347\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  3685.5490693929164\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  3685.546928355887\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  27806.05901054042\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  12833.62958113617\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  12487.68078718807\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  30165.53036038491\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  14928.465050736791\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  29855.559180364446\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  15743.480555946058\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  12302.276556594139\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  23866.7470883373\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  13625.822640604547\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  16812.30980274759\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  8313.039529653508\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  8313.039894227833\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  8313.039894227844\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  26086.078476107952\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  32357.97748639237\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  65193.46649339516\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  26184.99894173358\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  32358.39012452501\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  65193.466331389274\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  26184.99894218219\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  32358.39012452635\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  65193.466331389274\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  26184.99894218219\n",
      "\n",
      "========== Epoch 0 ==========> Train loss:  19665.149721095637\n",
      "\n",
      "========== Epoch 10 ==========> Train loss:  9268.910843508806\n",
      "\n",
      "========== Epoch 20 ==========> Train loss:  9269.387132403748\n",
      "\n",
      "========== Epoch 30 ==========> Train loss:  9269.397108855748\n",
      "\n",
      "========== Epoch 40 ==========> Train loss:  9269.397270265594\n",
      "\n",
      "========== Epoch 50 ==========> Train loss:  9269.397272867402\n",
      "\n",
      "========== Epoch 60 ==========> Train loss:  9269.397272909337\n",
      "\n",
      "========== Epoch 70 ==========> Train loss:  9269.397272910013\n",
      "\n",
      "========== Epoch 80 ==========> Train loss:  9269.397272910024\n",
      "\n",
      "========== Epoch 90 ==========> Train loss:  9269.397272910024\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(44)\n",
    "epochs = 100\n",
    "learnrate = 0.001\n",
    "\n",
    "def train(features,classes, epochs, learnrate):\n",
    "    final_weights=[]\n",
    "    for ind in range(0,9):\n",
    "        targets= classes==ind\n",
    "        targets=targets.astype(int)\n",
    "\n",
    "        errors = []\n",
    "        n_records, n_features = features.shape\n",
    "        last_loss = None\n",
    "        weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "        bias = 0\n",
    "        for e in range(epochs):\n",
    "            del_w = np.zeros(weights.shape)\n",
    "            for x, y in zip(features, targets):\n",
    "                output=sigmoid(np.dot(x,weights))\n",
    "                error_term=error_term_formula(x,y,output)\n",
    "                del_w+=error_term\n",
    "            weights+= learnrate*del_w\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean(error_formula(targets, out))\n",
    "            if e % (epochs / 10) == 0:\n",
    "                print(\"\\n========== Epoch\", e,\"==========> Train loss: \", loss)\n",
    "        final_weights.append(weights)\n",
    "    return(final_weights)\n",
    "\n",
    "final_weights=train(x,y,epochs,learnrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25.865\n"
     ]
    }
   ],
   "source": [
    "def cal_accuracy(features,classes,final_weights):\n",
    "    count=0\n",
    "    pridiction=np.zeros(len(classes))\n",
    "    for x,y in zip(features,classes):\n",
    "        final_output=[]\n",
    "        for ind in range(0,9):\n",
    "            weight=final_weights[ind]\n",
    "            bias=0\n",
    "            out = output_formula(x, weight, bias)\n",
    "            final_output.append(out)\n",
    "        label=np.argmax(final_output)\n",
    "        if label==y:\n",
    "            count+=1\n",
    "    #print(count)\n",
    "    return(count*100/len(classes))\n",
    "\n",
    "acc=cal_accuracy(x,y,final_weights)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"test.csv\")\n",
    "dfy=df1[\"id\"]\n",
    "dfx=df1.drop(columns=[\"id\"])\n",
    "dfx = dfx.astype(float)\n",
    "\n",
    "dfy=dfy.values.reshape(len(df1),1)\n",
    "dfy=np.array(dfy)\n",
    "\n",
    "dfx=np.divide(dfx,np.max(dfx,axis=0))\n",
    "dfx=np.array(dfx)\n",
    "df1=np.concatenate((dfy,dfx),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pridiction=pd.DataFrame(columns=[\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"])\n",
    "ids=pd.DataFrame()\n",
    "\n",
    "for x,y in zip(dfx,dfy):\n",
    "        final_output=[]\n",
    "        for ind in range(0,9):\n",
    "            weight=final_weights[ind]\n",
    "            bias=0\n",
    "            out = output_formula(x, weight, bias)\n",
    "            final_output.append(out)\n",
    "            #print(final_output)\n",
    "        a_series = pd.Series(final_output, index = pridiction.columns)\n",
    "        pridiction = pridiction.append(a_series, ignore_index=True)\n",
    "        #pridict=pd.DataFrame(final_output,columns=[\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"])\n",
    "        #pridiction.iloc[length]=final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pridiction=np.array(pridiction)\n",
    "pridiction=pridiction/pridiction.sum(axis=1, keepdims=True)\n",
    "pridiction=pd.DataFrame(pridiction,columns=[\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\",\"Class_5\",\"Class_6\",\"Class_7\",\"Class_8\",\"Class_9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfy=dfy.astype(int)\n",
    "dfy=pd.DataFrame(dfy,columns=[\"id\"])\n",
    "df=pd.concat([dfy,pridiction],axis=1)\n",
    "df.to_csv('final_pridictions_linear_reg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}