{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"c0f70214c0dd213f07f54ee5d6e0ea644bdbba35113c9bfe8aaa0d1db03ad5dd"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torchvision\n","\n","from torchvision import datasets, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","import time\n","\n","train_on_gpu = torch.cuda.is_available()\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:44:44.484615Z","iopub.execute_input":"2021-07-18T03:44:44.484941Z","iopub.status.idle":"2021-07-18T03:44:45.909246Z","shell.execute_reply.started":"2021-07-18T03:44:44.484911Z","shell.execute_reply":"2021-07-18T03:44:45.908300Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CUDA is available!  Training on GPU ...\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Load the data\n","Transform the data to the required size of the Xception model and also normalize it"],"metadata":{}},{"cell_type":"code","source":["test_datadir=\"../input/100-bird-species/birds/test\"\n","train_datadir=\"../input/100-bird-species/birds/train\"\n","valid_datadir=\"../input/100-bird-species/birds/valid\"\n","\n","transform=transforms.Compose([\n","    transforms.Resize([299,299]),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_data=datasets.ImageFolder(train_datadir,transform)\n","test_data=datasets.ImageFolder(test_datadir,transform)\n","valid_data=datasets.ImageFolder(valid_datadir,transform)\n","\n","batchsize=24\n","\n","train_dataloader=torch.utils.data.DataLoader(train_data, batch_size=batchsize, shuffle=True)\n","test_dataloader=torch.utils.data.DataLoader(test_data, batch_size=batchsize, shuffle=True)\n","valid_dataloader=torch.utils.data.DataLoader(valid_data, batch_size=batchsize, shuffle=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:44:47.781348Z","iopub.execute_input":"2021-07-18T03:44:47.781668Z","iopub.status.idle":"2021-07-18T03:44:56.851952Z","shell.execute_reply.started":"2021-07-18T03:44:47.781637Z","shell.execute_reply":"2021-07-18T03:44:56.851129Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Example display - \n","\n","img,label=next(iter(train_dataloader))\n","img = img.numpy()\n","img = img / 2 + 0.5 \n","plt.imshow(np.transpose(img[1], (2, 1, 0)))\n","print(label[1])"],"metadata":{"execution":{"iopub.status.busy":"2021-07-16T08:12:50.655336Z","iopub.execute_input":"2021-07-16T08:12:50.655688Z","iopub.status.idle":"2021-07-16T08:12:51.300109Z","shell.execute_reply.started":"2021-07-16T08:12:50.655655Z","shell.execute_reply":"2021-07-16T08:12:51.29932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Below we have used the Xception model - \n","\n","[--> Xception Paper link <--](https://paperswithcode.com/paper/xception-deep-learning-with-depthwise-1)"],"metadata":{}},{"cell_type":"code","source":["class EntryFlowLoop(nn.Module):\n","    def __init__(self,in_channels,out_channels,relu_start):\n","        super(EntryFlowLoop,self).__init__()\n","        self.relu_start=relu_start\n","        \n","        self.conv=nn.Conv2d(in_channels,out_channels,1,stride=2)\n","        self.bnc=nn.BatchNorm2d(out_channels)\n","        \n","        self.sc1a=nn.Conv2d(in_channels,in_channels,3,groups=in_channels,padding=1,bias=False)\n","        self.sc1b=nn.Conv2d(in_channels,out_channels,1,bias=False)\n","        self.bn1=nn.BatchNorm2d(out_channels)\n","        \n","        self.sc2a=nn.Conv2d(out_channels,out_channels,3,groups=out_channels,padding=1,bias=False)\n","        self.sc2b=nn.Conv2d(out_channels,out_channels,1,bias=False)\n","        self.bn2=nn.BatchNorm2d(out_channels)\n","        \n","        self.maxpool=nn.MaxPool2d(kernel_size=(3,3),stride=2,padding=1)\n","        self.relu=nn.ReLU()\n","    \n","    def forward(self,x):\n","                \n","        output=self.conv(x)\n","        output=self.bnc(output)\n","        \n","        if self.relu_start==True:\n","            x=self.relu(x)\n","        \n","        x=self.sc1b(self.sc1a(x))\n","        x=self.bn1(x)\n","        x=self.relu(x)\n","        \n","        x=self.sc2b(self.sc2a(x))\n","        x=self.bn2(x)\n","        x=self.maxpool(x)\n","        \n","        x+=output\n","        \n","        return x\n","\n","\n","class EntryFlowFinal(nn.Module):\n","    def __init__(self):\n","        super(EntryFlowFinal,self).__init__()\n","        \n","        self.c1=nn.Conv2d(3,32,3,stride=2)\n","        self.bn1=nn.BatchNorm2d(32)\n","\n","        self.c2=nn.Conv2d(32,64,3)\n","        self.bn2=nn.BatchNorm2d(64)\n","        \n","        self.loop1=EntryFlowLoop(64,128,False)\n","        self.loop2=EntryFlowLoop(128,256,True)\n","        self.loop3=EntryFlowLoop(256,728,True)\n","        \n","        self.relu=nn.ReLU()\n","    \n","    def forward(self,x):\n","        x=self.c1(x)\n","        x=self.bn1(x)\n","        x=self.relu(x)\n","        \n","        x=self.c2(x)\n","        x=self.bn2(x)\n","        x=self.relu(x)\n","        \n","        x=self.loop1(x)\n","        x=self.loop2(x)\n","        x=self.loop3(x)\n","        \n","        return x"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:44:59.167897Z","iopub.execute_input":"2021-07-18T03:44:59.168260Z","iopub.status.idle":"2021-07-18T03:44:59.183942Z","shell.execute_reply.started":"2021-07-18T03:44:59.168227Z","shell.execute_reply":"2021-07-18T03:44:59.182892Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class MiddleFlow(nn.Module):\n","    def __init__(self,channels=728):\n","        super(MiddleFlow,self).__init__()\n","        \n","        self.sc1a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n","        self.sc1b=nn.Conv2d(728,728,1,bias=False)\n","        \n","        self.sc2a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n","        self.sc2b=nn.Conv2d(728,728,1,bias=False)\n","        \n","        self.sc3a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n","        self.sc3b=nn.Conv2d(728,728,1,bias=False)\n","        \n","        \n","        self.bn1=nn.BatchNorm2d(728)\n","        self.bn2=nn.BatchNorm2d(728)\n","        self.bn3=nn.BatchNorm2d(728)\n","        self.relu=nn.ReLU()\n","        \n","    def forward(self,x):\n","        output=x\n","        \n","        x=self.sc1b(self.sc1a(self.relu(x)))\n","        x=self.bn1(x)\n","        x=self.sc2b(self.sc2a(self.relu(x)))\n","        x=self.bn2(x)\n","        x=self.sc3b(self.sc3a(self.relu(x)))\n","        x=self.bn3(x)\n","        \n","        x=x+output\n","        \n","        return x\n","\n","    \n","class MiddleFlowFinal(nn.Module):\n","    def __init__(self):\n","        super(MiddleFlowFinal,self).__init__()\n","        self.cycle1=MiddleFlow()\n","        self.cycle2=MiddleFlow()\n","        self.cycle3=MiddleFlow()\n","        self.cycle4=MiddleFlow()\n","        self.cycle5=MiddleFlow()\n","        self.cycle6=MiddleFlow()\n","        self.cycle7=MiddleFlow()\n","        self.cycle8=MiddleFlow()\n","    \n","    \n","    \n","    def forward(self,x):\n","#         for i in range(8):\n","#             x=self.cycle(x)\n","        x=self.cycle1(x)\n","        x=self.cycle2(x)\n","        x=self.cycle3(x)\n","        x=self.cycle4(x)\n","        x=self.cycle5(x)\n","        x=self.cycle6(x)\n","        x=self.cycle7(x)\n","        x=self.cycle8(x)\n","        \n","        return x"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:45:00.569449Z","iopub.execute_input":"2021-07-18T03:45:00.569769Z","iopub.status.idle":"2021-07-18T03:45:00.583531Z","shell.execute_reply.started":"2021-07-18T03:45:00.569739Z","shell.execute_reply":"2021-07-18T03:45:00.582310Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class ExitFlowFinal(nn.Module):\n","    def __init__(self):\n","        super(ExitFlowFinal,self).__init__()\n","        \n","        self.c1=nn.Conv2d(728,1024,1,stride=2)\n","        self.bnc=nn.BatchNorm2d(1024)\n","        \n","        self.sc1a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n","        self.sc1b=nn.Conv2d(728,728,1,bias=False)\n","        self.bn1=nn.BatchNorm2d(728)\n","        \n","        self.sc2a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n","        self.sc2b=nn.Conv2d(728,1024,1,bias=False)\n","        self.bn2=nn.BatchNorm2d(1024)\n","\n","        self.sc3a=nn.Conv2d(1024,1024,3,groups=1024,padding=1,bias=False)\n","        self.sc3b=nn.Conv2d(1024,1536,1,bias=False)\n","        self.bn3=nn.BatchNorm2d(1536)\n","\n","        self.sc4a=nn.Conv2d(1536,1536,3,groups=1536,padding=1,bias=False)\n","        self.sc4b=nn.Conv2d(1536,2048,1,bias=False)\n","        self.bn4=nn.BatchNorm2d(2048)\n","        \n","        self.maxpool=nn.MaxPool2d(kernel_size=(3,3),stride=2,padding=1)\n","        self.relu=nn.ReLU()\n","        self.avgpool=nn.AvgPool2d(kernel_size=(10,10))\n","        \n","    def forward(self,x):\n","        output=self.c1(x)\n","        output=self.bnc(output)\n","        \n","        x=self.relu(x)\n","        x=self.sc1b(self.sc1a(x))\n","        x=self.bn1(x)\n","        \n","        x=self.relu(x)\n","        x=self.sc2b(self.sc2a(x))\n","        x=self.bn2(x)\n","        \n","        x=self.maxpool(x)\n","        x+=output\n","        \n","        x=self.sc3b(self.sc3a(x))\n","        x=self.bn3(x)        \n","        x=self.relu(x)\n","        \n","        x=self.sc4b(self.sc4a(x))\n","        x=self.bn4(x)        \n","        x=self.relu(x)\n","        \n","        x=self.avgpool(x)\n","        \n","        return x   "],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:45:01.966513Z","iopub.execute_input":"2021-07-18T03:45:01.966837Z","iopub.status.idle":"2021-07-18T03:45:01.980831Z","shell.execute_reply.started":"2021-07-18T03:45:01.966808Z","shell.execute_reply":"2021-07-18T03:45:01.979971Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Xception_implementation(nn.Module):\n","    def __init__(self):\n","        super(Xception_implementation,self).__init__()       \n","        \n","        self.entry=EntryFlowFinal()\n","        self.middle=MiddleFlowFinal()\n","        self.exit=ExitFlowFinal()\n","        \n","        self.fc1=nn.Linear(2048,512)\n","        self.ol=nn.Linear(512,275)\n","        \n","        self.relu=nn.ReLU()\n","\n","    def forward(self,x):\n","        \n","        x=self.entry(x)\n","        x=self.middle(x)\n","        x=self.exit(x)\n","        \n","        x = x.view(x.size(0), -1) \n","        x=self.relu(self.fc1(x))\n","        x=self.relu(self.ol(x))\n","        return x\n","\n","\n","model = Xception_implementation()\n","#print(model)\n","\n","# move tensors to GPU if CUDA is available\n","if train_on_gpu:\n","    model.cuda()\n","! pip install torch-summary\n","from torchsummary import summary\n","summary(model,(3,299,299))"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:45:03.384979Z","iopub.execute_input":"2021-07-18T03:45:03.385338Z","iopub.status.idle":"2021-07-18T03:45:16.759500Z","shell.execute_reply.started":"2021-07-18T03:45:03.385306Z","shell.execute_reply":"2021-07-18T03:45:16.758666Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting torch-summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch-summary\nSuccessfully installed torch-summary-1.4.5\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─EntryFlowFinal: 1-1                    [-1, 728, 19, 19]         --\n|    └─Conv2d: 2-1                       [-1, 32, 149, 149]        896\n|    └─BatchNorm2d: 2-2                  [-1, 32, 149, 149]        64\n|    └─ReLU: 2-3                         [-1, 32, 149, 149]        --\n|    └─Conv2d: 2-4                       [-1, 64, 147, 147]        18,496\n|    └─BatchNorm2d: 2-5                  [-1, 64, 147, 147]        128\n|    └─ReLU: 2-6                         [-1, 64, 147, 147]        --\n|    └─EntryFlowLoop: 2-7                [-1, 128, 74, 74]         --\n|    |    └─Conv2d: 3-1                  [-1, 128, 74, 74]         8,320\n|    |    └─BatchNorm2d: 3-2             [-1, 128, 74, 74]         256\n|    |    └─Conv2d: 3-3                  [-1, 64, 147, 147]        576\n|    |    └─Conv2d: 3-4                  [-1, 128, 147, 147]       8,192\n|    |    └─BatchNorm2d: 3-5             [-1, 128, 147, 147]       256\n|    |    └─ReLU: 3-6                    [-1, 128, 147, 147]       --\n|    |    └─Conv2d: 3-7                  [-1, 128, 147, 147]       1,152\n|    |    └─Conv2d: 3-8                  [-1, 128, 147, 147]       16,384\n|    |    └─BatchNorm2d: 3-9             [-1, 128, 147, 147]       256\n|    |    └─MaxPool2d: 3-10              [-1, 128, 74, 74]         --\n|    └─EntryFlowLoop: 2-8                [-1, 256, 37, 37]         --\n|    |    └─Conv2d: 3-11                 [-1, 256, 37, 37]         33,024\n|    |    └─BatchNorm2d: 3-12            [-1, 256, 37, 37]         512\n|    |    └─ReLU: 3-13                   [-1, 128, 74, 74]         --\n|    |    └─Conv2d: 3-14                 [-1, 128, 74, 74]         1,152\n|    |    └─Conv2d: 3-15                 [-1, 256, 74, 74]         32,768\n|    |    └─BatchNorm2d: 3-16            [-1, 256, 74, 74]         512\n|    |    └─ReLU: 3-17                   [-1, 256, 74, 74]         --\n|    |    └─Conv2d: 3-18                 [-1, 256, 74, 74]         2,304\n|    |    └─Conv2d: 3-19                 [-1, 256, 74, 74]         65,536\n|    |    └─BatchNorm2d: 3-20            [-1, 256, 74, 74]         512\n|    |    └─MaxPool2d: 3-21              [-1, 256, 37, 37]         --\n|    └─EntryFlowLoop: 2-9                [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-22                 [-1, 728, 19, 19]         187,096\n|    |    └─BatchNorm2d: 3-23            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-24                   [-1, 256, 37, 37]         --\n|    |    └─Conv2d: 3-25                 [-1, 256, 37, 37]         2,304\n|    |    └─Conv2d: 3-26                 [-1, 728, 37, 37]         186,368\n|    |    └─BatchNorm2d: 3-27            [-1, 728, 37, 37]         1,456\n|    |    └─ReLU: 3-28                   [-1, 728, 37, 37]         --\n|    |    └─Conv2d: 3-29                 [-1, 728, 37, 37]         6,552\n|    |    └─Conv2d: 3-30                 [-1, 728, 37, 37]         529,984\n|    |    └─BatchNorm2d: 3-31            [-1, 728, 37, 37]         1,456\n|    |    └─MaxPool2d: 3-32              [-1, 728, 19, 19]         --\n├─MiddleFlowFinal: 1-2                   [-1, 728, 19, 19]         --\n|    └─MiddleFlow: 2-10                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-33                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-34                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-35                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-36            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-37                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-38                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-39                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-40            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-41                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-42                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-43                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-44            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-11                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-45                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-46                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-47                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-48            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-49                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-50                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-51                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-52            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-53                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-54                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-55                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-56            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-12                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-57                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-58                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-59                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-60            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-61                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-62                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-63                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-64            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-65                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-66                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-67                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-68            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-13                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-69                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-70                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-71                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-72            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-73                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-74                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-75                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-76            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-77                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-78                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-79                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-80            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-14                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-81                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-82                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-83                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-84            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-85                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-86                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-87                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-88            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-89                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-90                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-91                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-92            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-15                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-93                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-94                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-95                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-96            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-97                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-98                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-99                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-100           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-101                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-102                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-103                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-104           [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-16                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-105                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-106                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-107                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-108           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-109                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-110                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-111                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-112           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-113                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-114                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-115                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-116           [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-17                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-117                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-118                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-119                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-120           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-121                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-122                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-123                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-124           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-125                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-126                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-127                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-128           [-1, 728, 19, 19]         1,456\n├─ExitFlowFinal: 1-3                     [-1, 2048, 1, 1]          --\n|    └─Conv2d: 2-18                      [-1, 1024, 10, 10]        746,496\n|    └─BatchNorm2d: 2-19                 [-1, 1024, 10, 10]        2,048\n|    └─ReLU: 2-20                        [-1, 728, 19, 19]         --\n|    └─Conv2d: 2-21                      [-1, 728, 19, 19]         6,552\n|    └─Conv2d: 2-22                      [-1, 728, 19, 19]         529,984\n|    └─BatchNorm2d: 2-23                 [-1, 728, 19, 19]         1,456\n|    └─ReLU: 2-24                        [-1, 728, 19, 19]         --\n|    └─Conv2d: 2-25                      [-1, 728, 19, 19]         6,552\n|    └─Conv2d: 2-26                      [-1, 1024, 19, 19]        745,472\n|    └─BatchNorm2d: 2-27                 [-1, 1024, 19, 19]        2,048\n|    └─MaxPool2d: 2-28                   [-1, 1024, 10, 10]        --\n|    └─Conv2d: 2-29                      [-1, 1024, 10, 10]        9,216\n|    └─Conv2d: 2-30                      [-1, 1536, 10, 10]        1,572,864\n|    └─BatchNorm2d: 2-31                 [-1, 1536, 10, 10]        3,072\n|    └─ReLU: 2-32                        [-1, 1536, 10, 10]        --\n|    └─Conv2d: 2-33                      [-1, 1536, 10, 10]        13,824\n|    └─Conv2d: 2-34                      [-1, 2048, 10, 10]        3,145,728\n|    └─BatchNorm2d: 2-35                 [-1, 2048, 10, 10]        4,096\n|    └─ReLU: 2-36                        [-1, 2048, 10, 10]        --\n|    └─AvgPool2d: 2-37                   [-1, 2048, 1, 1]          --\n├─Linear: 1-4                            [-1, 512]                 1,049,088\n├─ReLU: 1-5                              [-1, 512]                 --\n├─Linear: 1-6                            [-1, 275]                 141,075\n├─ReLU: 1-7                              [-1, 275]                 --\n==========================================================================================\nTotal params: 21,999,347\nTrainable params: 21,999,347\nNon-trainable params: 0\nTotal mult-adds (G): 8.39\n==========================================================================================\nInput size (MB): 1.02\nForward/backward pass size (MB): 434.59\nParams size (MB): 83.92\nEstimated Total Size (MB): 519.54\n==========================================================================================\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─EntryFlowFinal: 1-1                    [-1, 728, 19, 19]         --\n|    └─Conv2d: 2-1                       [-1, 32, 149, 149]        896\n|    └─BatchNorm2d: 2-2                  [-1, 32, 149, 149]        64\n|    └─ReLU: 2-3                         [-1, 32, 149, 149]        --\n|    └─Conv2d: 2-4                       [-1, 64, 147, 147]        18,496\n|    └─BatchNorm2d: 2-5                  [-1, 64, 147, 147]        128\n|    └─ReLU: 2-6                         [-1, 64, 147, 147]        --\n|    └─EntryFlowLoop: 2-7                [-1, 128, 74, 74]         --\n|    |    └─Conv2d: 3-1                  [-1, 128, 74, 74]         8,320\n|    |    └─BatchNorm2d: 3-2             [-1, 128, 74, 74]         256\n|    |    └─Conv2d: 3-3                  [-1, 64, 147, 147]        576\n|    |    └─Conv2d: 3-4                  [-1, 128, 147, 147]       8,192\n|    |    └─BatchNorm2d: 3-5             [-1, 128, 147, 147]       256\n|    |    └─ReLU: 3-6                    [-1, 128, 147, 147]       --\n|    |    └─Conv2d: 3-7                  [-1, 128, 147, 147]       1,152\n|    |    └─Conv2d: 3-8                  [-1, 128, 147, 147]       16,384\n|    |    └─BatchNorm2d: 3-9             [-1, 128, 147, 147]       256\n|    |    └─MaxPool2d: 3-10              [-1, 128, 74, 74]         --\n|    └─EntryFlowLoop: 2-8                [-1, 256, 37, 37]         --\n|    |    └─Conv2d: 3-11                 [-1, 256, 37, 37]         33,024\n|    |    └─BatchNorm2d: 3-12            [-1, 256, 37, 37]         512\n|    |    └─ReLU: 3-13                   [-1, 128, 74, 74]         --\n|    |    └─Conv2d: 3-14                 [-1, 128, 74, 74]         1,152\n|    |    └─Conv2d: 3-15                 [-1, 256, 74, 74]         32,768\n|    |    └─BatchNorm2d: 3-16            [-1, 256, 74, 74]         512\n|    |    └─ReLU: 3-17                   [-1, 256, 74, 74]         --\n|    |    └─Conv2d: 3-18                 [-1, 256, 74, 74]         2,304\n|    |    └─Conv2d: 3-19                 [-1, 256, 74, 74]         65,536\n|    |    └─BatchNorm2d: 3-20            [-1, 256, 74, 74]         512\n|    |    └─MaxPool2d: 3-21              [-1, 256, 37, 37]         --\n|    └─EntryFlowLoop: 2-9                [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-22                 [-1, 728, 19, 19]         187,096\n|    |    └─BatchNorm2d: 3-23            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-24                   [-1, 256, 37, 37]         --\n|    |    └─Conv2d: 3-25                 [-1, 256, 37, 37]         2,304\n|    |    └─Conv2d: 3-26                 [-1, 728, 37, 37]         186,368\n|    |    └─BatchNorm2d: 3-27            [-1, 728, 37, 37]         1,456\n|    |    └─ReLU: 3-28                   [-1, 728, 37, 37]         --\n|    |    └─Conv2d: 3-29                 [-1, 728, 37, 37]         6,552\n|    |    └─Conv2d: 3-30                 [-1, 728, 37, 37]         529,984\n|    |    └─BatchNorm2d: 3-31            [-1, 728, 37, 37]         1,456\n|    |    └─MaxPool2d: 3-32              [-1, 728, 19, 19]         --\n├─MiddleFlowFinal: 1-2                   [-1, 728, 19, 19]         --\n|    └─MiddleFlow: 2-10                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-33                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-34                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-35                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-36            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-37                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-38                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-39                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-40            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-41                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-42                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-43                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-44            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-11                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-45                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-46                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-47                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-48            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-49                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-50                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-51                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-52            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-53                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-54                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-55                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-56            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-12                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-57                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-58                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-59                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-60            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-61                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-62                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-63                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-64            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-65                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-66                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-67                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-68            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-13                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-69                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-70                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-71                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-72            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-73                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-74                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-75                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-76            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-77                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-78                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-79                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-80            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-14                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-81                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-82                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-83                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-84            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-85                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-86                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-87                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-88            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-89                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-90                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-91                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-92            [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-15                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-93                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-94                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-95                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-96            [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-97                   [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-98                 [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-99                 [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-100           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-101                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-102                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-103                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-104           [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-16                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-105                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-106                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-107                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-108           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-109                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-110                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-111                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-112           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-113                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-114                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-115                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-116           [-1, 728, 19, 19]         1,456\n|    └─MiddleFlow: 2-17                  [-1, 728, 19, 19]         --\n|    |    └─ReLU: 3-117                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-118                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-119                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-120           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-121                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-122                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-123                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-124           [-1, 728, 19, 19]         1,456\n|    |    └─ReLU: 3-125                  [-1, 728, 19, 19]         --\n|    |    └─Conv2d: 3-126                [-1, 728, 19, 19]         6,552\n|    |    └─Conv2d: 3-127                [-1, 728, 19, 19]         529,984\n|    |    └─BatchNorm2d: 3-128           [-1, 728, 19, 19]         1,456\n├─ExitFlowFinal: 1-3                     [-1, 2048, 1, 1]          --\n|    └─Conv2d: 2-18                      [-1, 1024, 10, 10]        746,496\n|    └─BatchNorm2d: 2-19                 [-1, 1024, 10, 10]        2,048\n|    └─ReLU: 2-20                        [-1, 728, 19, 19]         --\n|    └─Conv2d: 2-21                      [-1, 728, 19, 19]         6,552\n|    └─Conv2d: 2-22                      [-1, 728, 19, 19]         529,984\n|    └─BatchNorm2d: 2-23                 [-1, 728, 19, 19]         1,456\n|    └─ReLU: 2-24                        [-1, 728, 19, 19]         --\n|    └─Conv2d: 2-25                      [-1, 728, 19, 19]         6,552\n|    └─Conv2d: 2-26                      [-1, 1024, 19, 19]        745,472\n|    └─BatchNorm2d: 2-27                 [-1, 1024, 19, 19]        2,048\n|    └─MaxPool2d: 2-28                   [-1, 1024, 10, 10]        --\n|    └─Conv2d: 2-29                      [-1, 1024, 10, 10]        9,216\n|    └─Conv2d: 2-30                      [-1, 1536, 10, 10]        1,572,864\n|    └─BatchNorm2d: 2-31                 [-1, 1536, 10, 10]        3,072\n|    └─ReLU: 2-32                        [-1, 1536, 10, 10]        --\n|    └─Conv2d: 2-33                      [-1, 1536, 10, 10]        13,824\n|    └─Conv2d: 2-34                      [-1, 2048, 10, 10]        3,145,728\n|    └─BatchNorm2d: 2-35                 [-1, 2048, 10, 10]        4,096\n|    └─ReLU: 2-36                        [-1, 2048, 10, 10]        --\n|    └─AvgPool2d: 2-37                   [-1, 2048, 1, 1]          --\n├─Linear: 1-4                            [-1, 512]                 1,049,088\n├─ReLU: 1-5                              [-1, 512]                 --\n├─Linear: 1-6                            [-1, 275]                 141,075\n├─ReLU: 1-7                              [-1, 275]                 --\n==========================================================================================\nTotal params: 21,999,347\nTrainable params: 21,999,347\nNon-trainable params: 0\nTotal mult-adds (G): 8.39\n==========================================================================================\nInput size (MB): 1.02\nForward/backward pass size (MB): 434.59\nParams size (MB): 83.92\nEstimated Total Size (MB): 519.54\n=========================================================================================="},"metadata":{}}]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# specify loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# specify optimizer\n","optimizer = optim.SGD(model.parameters(),lr=0.03)\n","#optimizer = optim.Adam(model.parameters(),lr=0.00005)"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:45:19.118695Z","iopub.execute_input":"2021-07-18T03:45:19.119028Z","iopub.status.idle":"2021-07-18T03:45:19.127140Z","shell.execute_reply.started":"2021-07-18T03:45:19.118994Z","shell.execute_reply":"2021-07-18T03:45:19.125810Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["n_epochs = 10 \n","valid_loss_min = np.Inf # track change in validation loss\n","batch_number=0\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    \n","    for data, target in train_dataloader:\n","        batch_number+=1\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","\n","        \n","        loss = criterion(output, target)\n","\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        \n","        if batch_number%100==0:\n","            print(batch_number)\n","            #print(train_loss/(24*batch_number))\n","    ######################    \n","    # validate the model #\n","    ######################\n","    model.eval()\n","    for data, target in valid_dataloader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_dataloader.dataset)\n","    valid_loss = valid_loss/len(valid_dataloader.dataset)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(model.state_dict(), 'model_bird_species.pt')\n","        valid_loss_min = valid_loss"],"metadata":{"execution":{"iopub.status.busy":"2021-07-16T12:43:31.044311Z","iopub.execute_input":"2021-07-16T12:43:31.044663Z","iopub.status.idle":"2021-07-16T12:48:48.103574Z","shell.execute_reply.started":"2021-07-16T12:43:31.044625Z","shell.execute_reply":"2021-07-16T12:48:48.102003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adjusting different parameters - \n","We are using Xception paper model and therefore the hyper-parameters (number of layers and their size) is fixed. Therefore we are left with the following parameters- \n","1. Optimizer- We will try 2 -  Adam and SGD (with momentum)\n","2. lr - We will try different ranges of lr to get to most suitable value\n","3. batch-size - Though I have a little problem of memory issues and theefore I cannot try a large range of Batch-size, but still I try to achieve the sweet spot.\n","4. Logistic NN after the Xception part of the Neural Network\n","\n","#### For lr=0.01, optimizer= Adam, batch_size=24 -\n","100  \n","5.68693591594696  \n","200  \n","5.6518528532981875  \n","300  \n","5.640158499081929  \n","400  \n","5.634311321973801  \n","500  \n","5.630803015708923  \n","\n","##### Conclusion:  \n","The values are reaching the limit very early, therefore its possible that our ls is very large due to which its just oscillating around a local minima or its very small that its not moving significantly.  \n","We will first try to increase the lr.\n","  \n","\n","#### For lr=0.03, optimizer= Adam, batch_size=24 -\n","\n","100  \n","6.984280848503113  \n","200  \n","6.300525319576264  \n","300  \n","6.072606809933981  \n","400  \n","5.958647555112838  \n","500  \n","5.890272002220154  \n","\n","##### Conclusion:  \n","Though its a bad performance campared to the previous test, therefore we conclude that our lr is very large for the data. But, to double confirm this, we will run it one more time with larger lr.\n","  \n","  \n","#### For lr=0.09, optimizer= Adam, batch_size=24 -\n","100  \n","17.518945121765135  \n","200  \n","11.567857458591462  \n","300  \n","9.584161569277445   \n","\n","##### Conclusion:  \n","Our caonclusion is confirmed and now we will run many different tests, decreasing the lr slowly, till we reach a sweet spot.\n","\n","\n","#### For lr=0.005, optimizer= Adam, batch_size=24 -\n","\n","100  \n","5.628419208526611  \n","200  \n","5.622594499588013  \n","300  \n","5.620652929941813  \n","400  \n","5.619682145118714  \n","500  \n","5.619099674224853  \n","600  \n","5.618711360295614  \n","700  \n","5.6184339932033  \n","800  \n","5.618225967884063  \n","900  \n","5.618064170413547  \n","1000  \n","5.617934732437134  \n","\n","##### Conclusion:  \n","We are seeing improvements.  \n","Lets keep decreasing the lr.\n","  \n","#### For lr=0.001, optimizer= Adam, batch_size=24 -\n","\n","100  \n","5.596344637870788  \n","200  \n","5.5770020365715025  \n","300  \n","5.557831103006999  \n","400  \n","5.545575157403946  \n","500  \n","5.537669966697693  \n","\n","\n","##### Conclusion:  \n","We are seeing improvements.  \n","Lets keep decreasing the lr.\n","\n","#### For lr=0.0005, optimizer= Adam, batch_size=24 -\n","\n","100  \n","5.615420479774475  \n","200  \n","5.6130558156967165  \n","300  \n","5.613888082504272  \n","400  \n","5.613553690910339  \n","500  \n","5.612753263473511   \n","\n","##### Conclusion:  \n","Its a rough spot. Maybe lr=0.001 was better. But its almost the same results, and we should try decreasing the lr again.\n","\n","#### For lr=0.0001, optimizer= Adam, batch_size=24 -\n","100  \n","5.594290690422058  \n","200  \n","5.586528244018555  \n","300  \n","5.56579353650411  \n","400  \n","5.5533226096630095  \n","500  \n","5.540951251029968  \n","600  \n","5.531232864061991  \n","\n","##### Conclusion:  \n","Definately a big improvement. Also notice that the decrease in loss with 100 batches is also increasing.  \n","This means we are on right track and lets keep reducing the lr.\n","\n","#### For lr=0.00005, optimizer= Adam, batch_size=24 -\n","\n","100  \n","5.604961247444153  \n","200  \n","5.578570282459259  \n","300  \n","5.552625319163004  \n","400  \n","5.532134653329849  \n","500  \n","5.508835887908935  \n","\n","##### Conclusion:  \n","We are seeing improvements.  \n","Lets keep decreasing the lr.\n","\n","#### For lr=0.00001, optimizer= Adam, batch_size=24 -\n","\n","100  \n","5.611733942031861  \n","200  \n","5.603807361125946  \n","300  \n","5.59537739276886  \n","400  \n","5.585271533727646  \n","500  \n","5.574601455688477  \n","600  \n","5.563503699302673   \n","\n","##### Conclusion:  \n","Though not a bad performance, but lr=0.00005 was better than 0.00001 and therefore we conclude that the reducing lr further is not good.\n","\n","## Therefore Lets fix lr at 0.00005  \n","\n","#### Now lets focus on Logistic NN part just after the Xception layers. Till now, we were using 2 layers - (2048,512,275)  \n","\n","#### For lr=0.00005, optimizer= Adam, batch_size=24  and NN ( 1 layer )-\n","100  \n","5.596523351669312  \n","200  \n","5.574898626804352  \n","300  \n","5.565456638336181  \n","400  \n","5.554475079774857  \n","500  \n","5.537313059806824  \n","\n","##### Conclusion:  \n","Performance has definately gone down.  \n","Instead of decreasing, lets try to increase the number of layers.\n","\n","#### For lr=0.00005, optimizer= Adam, batch_size=24  and NN ( 3 layers - 2048,1024,512,275 )-\n","\n","100  \n","5.606496725082398  \n","200  \n","5.5884935927391055  \n","300  \n","5.564859512646993  \n","400  \n","5.549061695337295  \n","500  \n","5.530344123840332  \n","600  \n","5.517454077402751  \n","\n","##### Conclusion:  \n","Nope.  \n","The performance has again gone down.  \n","2 layered network was better.  \n","\n","## Therefore, lets keep the NN as 2 layered - 2048,512,275\n","\n","#### Now lets focus on the optimizer and try SGD with different lr.  \n","#### For lr=0.00005, optimizer= SGD, batch_size=24 -\n","100\n","5.618415269851685\n","200\n","5.617383131980896\n","300\n","5.617286367416382\n","400\n","5.617484185695648\n","500\n","5.61704674911499  \n","\n","##### Conclusion:  \n","Not a good performance compared to Adam.  \n","Maybe changing the lr will help.  \n","Lets increase the lr.\n","\n","#### For lr=0.0001, optimizer= SGD, batch_size=24 -\n","100  \n","5.620724911689758  \n","200  \n","5.619467494487762  \n","300  \n","5.617840498288473  \n","400  \n","5.617654368877411  \n","500  \n","5.6167002849578855  \n","600  \n","5.615881765683492  \n","\n","##### Conclusion:  \n","Almost the same performance as the previous one.  \n","Lets increse the lr more and compare the results.\n","\n","#### For lr=0.005, optimizer= SGD, batch_size=24 -\n","100  \n","5.614914889335632  \n","200  \n","5.611899130344391  \n","300  \n","5.610926251411438  \n","400  \n","5.609055856466293  \n","500  \n","5.606855712890625  \n","\n","##### Conclusion:  \n","Though its an improvement.  \n","But we can see that the change in loss in very less and therefore in a few more steps it will change negligibly.  \n","\n","#### For lr=0.03, optimizer= SGD, batch_size=24 -\n","\n","100  \n","5.604961247444153  \n","200  \n","5.598570282459259  \n","300  \n","5.582625319163004  \n","400  \n","5.572134653329849  \n","500  \n","5.568835887908935  \n","\n","##### Conclusion:  \n","Though its not as good as the Adam 0.00005 but its close.  \n","Also, this lr is very large compared to 0.00005 this will help us with faster convergence.  \n","Though I claim that on running a lot of epochs, Adam model will give us better accuracy but, this model will be more praticle.    \n","\n","\n","\n","## Therefore, its better if we use SGD with lr=0.03.  \n","\n","\n","NOTE : I tried changing the batch-size but it had almost no effect on values, moreover, increasing it to more than 24 is giving Memory issues and therefore, we will remain with 24 as the batch-size.  \n","Therefore, the final choosen values are-  \n","1. lr = 0.00005\n","2. Logictic NN after Xception - 2 layers (2048,512,275)\n","3. Optimizer- Adam\n","4. Batch-size = 24"],"metadata":{}},{"cell_type":"code","source":["torch.save(model.state_dict(), 'model_bird_species_2.pt')"],"metadata":{"execution":{"iopub.status.busy":"2021-07-15T16:37:05.885598Z","iopub.execute_input":"2021-07-15T16:37:05.886027Z","iopub.status.idle":"2021-07-15T16:37:05.993597Z","shell.execute_reply.started":"2021-07-15T16:37:05.885997Z","shell.execute_reply":"2021-07-15T16:37:05.992577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing \n","We have trained and saved 2 models -\n","1. model_bird_species - is the model where we are getting the min Validation loss ( but high training loss)\n","2. model_bird_species_2 - is the model where are fitting the data efficiently to the training data but have high loss in validation data  \n","\n","Choose 1 saved model in the below code to see the what accuracy we are getting on each model."],"metadata":{}},{"cell_type":"code","source":["# Choose 1 from the below 2-\n","model.load_state_dict(torch.load('../input/saved-model-bird-species/model_bird_species.pt'))\n","#model.load_state_dict(torch.load('../input/saved-model-bird-species/model_bird_species_2.pt'))\n","\n","test_loss = 0.0\n","class_correct = list(0. for i in range(275))\n","class_total = list(0. for i in range(275))\n","\n","model.eval()\n","# iterate over test data\n","for data, target in test_dataloader:\n","    \n","    if train_on_gpu:\n","        data, target = data.cuda(), target.cuda()\n","    \n","    output = model(data)    \n","    loss = criterion(output, target)\n","    \n","    test_loss += loss.item()*data.size(0)\n","    \n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    # calculate test accuracy for each object class\n","    batchsize=data.size(0)\n","    for i in range(batchsize):\n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# average test loss\n","test_loss = test_loss/len(test_dataloader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(275):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of class %5s: %2d%% (%2d/%2d)' % (\n","            i, 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))"],"metadata":{"execution":{"iopub.status.busy":"2021-07-16T04:13:19.597784Z","iopub.execute_input":"2021-07-16T04:13:19.59816Z","iopub.status.idle":"2021-07-16T04:13:40.372504Z","shell.execute_reply.started":"2021-07-16T04:13:19.598129Z","shell.execute_reply":"2021-07-16T04:13:40.371725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Observation -\n","We saw that we reached the best possible validation loss in the first few epochs and the trainig loss kept on decreasing.  \n","Which is not very good as we want to reduce the validation loss and not necessarily the training loss.  \n","\n","Maybe by randomising and making the training data more general, we may get a better validation loss (at the cost of training loss)  \n","\n","Therefore, we add randomisation to our training data -"],"metadata":{}},{"cell_type":"code","source":["transform_2 = transforms.Compose([transforms.RandomRotation(30),\n","                                       transforms.RandomResizedCrop(299),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.5, 0.5, 0.5], \n","                                                            [0.5, 0.5, 0.5])])\n","\n","train_data_2=datasets.ImageFolder(train_datadir,transform_2)\n","\n","train_dataloader_2=torch.utils.data.DataLoader(train_data_2, batch_size=batchsize, shuffle=True)\n"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:45:37.741180Z","iopub.execute_input":"2021-07-18T03:45:37.741552Z","iopub.status.idle":"2021-07-18T03:45:38.257252Z","shell.execute_reply.started":"2021-07-18T03:45:37.741522Z","shell.execute_reply":"2021-07-18T03:45:38.256377Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Now lets train the new data again"],"metadata":{}},{"cell_type":"code","source":["n_epochs = 100 \n","valid_loss_min = np.Inf # track change in validation loss\n","batch_number=0\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    \n","    for data, target in train_dataloader_2:\n","        batch_number+=1\n","        \n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        \n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","\n","        \n","        loss = criterion(output, target)\n","\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        \n","        if batch_number%100==0:\n","            print(batch_number)\n","            #print(train_loss/(24*batch_number))\n","    ######################    \n","    # validate the model #\n","    ######################\n","    model.eval()\n","    for data, target in valid_dataloader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_dataloader_2.dataset)\n","    valid_loss = valid_loss/len(valid_dataloader.dataset)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(model.state_dict(), 'model_bird_species_3.pt')\n","        valid_loss_min = valid_loss"],"metadata":{"execution":{"iopub.status.busy":"2021-07-17T13:34:56.505087Z","iopub.execute_input":"2021-07-17T13:34:56.505457Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\nEpoch: 1 \tTraining Loss: 5.324159 \tValidation Loss: 5.140066\nValidation loss decreased (inf --> 5.140066).  Saving model ...\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\nEpoch: 2 \tTraining Loss: 4.754307 \tValidation Loss: 4.985553\nValidation loss decreased (5.140066 --> 4.985553).  Saving model ...\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\nEpoch: 3 \tTraining Loss: 4.329358 \tValidation Loss: 4.318512\nValidation loss decreased (4.985553 --> 4.318512).  Saving model ...\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\nEpoch: 4 \tTraining Loss: 3.881392 \tValidation Loss: 3.613247\nValidation loss decreased (4.318512 --> 3.613247).  Saving model ...\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\nEpoch: 5 \tTraining Loss: 3.501989 \tValidation Loss: 3.935021\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\nEpoch: 6 \tTraining Loss: 3.164692 \tValidation Loss: 2.785035\nValidation loss decreased (3.613247 --> 2.785035).  Saving model ...\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\nEpoch: 7 \tTraining Loss: 2.914993 \tValidation Loss: 3.187215\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\nEpoch: 8 \tTraining Loss: 2.726420 \tValidation Loss: 2.623250\nValidation loss decreased (2.785035 --> 2.623250).  Saving model ...\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n14300\n14400\n14500\n14600\n14700\nEpoch: 9 \tTraining Loss: 2.554163 \tValidation Loss: 2.249233\nValidation loss decreased (2.623250 --> 2.249233).  Saving model ...\n14800\n14900\n15000\n15100\n15200\n15300\n15400\n15500\n15600\n15700\n15800\n15900\n16000\n16100\n16200\n16300\n16400\nEpoch: 10 \tTraining Loss: 2.385756 \tValidation Loss: 1.716781\nValidation loss decreased (2.249233 --> 1.716781).  Saving model ...\n16500\n16600\n16700\n16800\n16900\n17000\n17100\n17200\n17300\n17400\n17500\n17600\n17700\n17800\n17900\n18000\nEpoch: 11 \tTraining Loss: 2.248410 \tValidation Loss: 2.308897\n18100\n18200\n18300\n18400\n18500\n18600\n18700\n18800\n18900\n19000\n19100\n19200\n19300\n19400\n19500\n19600\nEpoch: 12 \tTraining Loss: 2.121537 \tValidation Loss: 1.552101\nValidation loss decreased (1.716781 --> 1.552101).  Saving model ...\n19700\n19800\n19900\n20000\n20100\n20200\n20300\n20400\n20500\n20600\n20700\n20800\n20900\n21000\n21100\n21200\n21300\nEpoch: 13 \tTraining Loss: 2.019976 \tValidation Loss: 1.811825\n21400\n21500\n21600\n21700\n21800\n21900\n22000\n22100\n22200\n22300\n22400\n22500\n22600\n22700\n22800\n22900\nEpoch: 14 \tTraining Loss: 1.932086 \tValidation Loss: 1.198414\nValidation loss decreased (1.552101 --> 1.198414).  Saving model ...\n23000\n23100\n23200\n23300\n23400\n23500\n23600\n23700\n23800\n23900\n24000\n24100\n24200\n24300\n24400\n24500\n24600\nEpoch: 15 \tTraining Loss: 1.833350 \tValidation Loss: 1.209750\n24700\n24800\n24900\n25000\n25100\n25200\n25300\n25400\n25500\n25600\n25700\n25800\n25900\n26000\n26100\n26200\nEpoch: 16 \tTraining Loss: 1.739494 \tValidation Loss: 1.938908\n26300\n26400\n26500\n26600\n26700\n26800\n26900\n27000\n27100\n27200\n27300\n27400\n27500\n27600\n27700\n27800\nEpoch: 17 \tTraining Loss: 1.687809 \tValidation Loss: 1.262992\n27900\n28000\n28100\n28200\n28300\n28400\n28500\n28600\n28700\n28800\n28900\n29000\n29100\n29200\n29300\n29400\n29500\nEpoch: 18 \tTraining Loss: 1.612250 \tValidation Loss: 1.936381\n29600\n29700\n29800\n29900\n30000\n30100\n30200\n30300\n30400\n30500\n30600\n30700\n30800\n30900\n31000\n31100\nEpoch: 19 \tTraining Loss: 1.547523 \tValidation Loss: 2.410224\n31200\n31300\n31400\n31500\n31600\n31700\n31800\n31900\n32000\n32100\n32200\n32300\n32400\n32500\n32600\n32700\n32800\nEpoch: 20 \tTraining Loss: 1.475699 \tValidation Loss: 0.899197\nValidation loss decreased (1.198414 --> 0.899197).  Saving model ...\n32900\n33000\n33100\n33200\n33300\n33400\n33500\n33600\n33700\n33800\n33900\n34000\n34100\n34200\n34300\n34400\nEpoch: 21 \tTraining Loss: 1.437210 \tValidation Loss: 0.825283\nValidation loss decreased (0.899197 --> 0.825283).  Saving model ...\n34500\n34600\n34700\n34800\n34900\n35000\n35100\n35200\n35300\n35400\n35500\n35600\n35700\n35800\n35900\n36000\n36100\nEpoch: 22 \tTraining Loss: 1.380561 \tValidation Loss: 0.997007\n36200\n36300\n36400\n36500\n36600\n36700\n36800\n36900\n37000\n37100\n37200\n37300\n37400\n37500\n37600\n37700\nEpoch: 23 \tTraining Loss: 1.345732 \tValidation Loss: 0.980332\n37800\n37900\n38000\n38100\n38200\n38300\n38400\n38500\n38600\n38700\n38800\n38900\n39000\n39100\n39200\n39300\nEpoch: 24 \tTraining Loss: 1.311146 \tValidation Loss: 2.019247\n39400\n39500\n39600\n39700\n39800\n39900\n40000\n40100\n40200\n40300\n40400\n40500\n40600\n40700\n40800\n40900\n41000\nEpoch: 25 \tTraining Loss: 1.295448 \tValidation Loss: 0.816486\nValidation loss decreased (0.825283 --> 0.816486).  Saving model ...\n41100\n41200\n41300\n41400\n41500\n41600\n41700\n41800\n41900\n42000\n42100\n42200\n42300\n42400\n42500\n42600\nEpoch: 26 \tTraining Loss: 1.202554 \tValidation Loss: 0.698600\nValidation loss decreased (0.816486 --> 0.698600).  Saving model ...\n42700\n42800\n42900\n43000\n43100\n43200\n43300\n43400\n43500\n43600\n43700\n43800\n43900\n44000\n44100\n44200\n44300\nEpoch: 27 \tTraining Loss: 1.182559 \tValidation Loss: 0.674529\nValidation loss decreased (0.698600 --> 0.674529).  Saving model ...\n44400\n44500\n44600\n44700\n44800\n44900\n45000\n45100\n45200\n45300\n45400\n45500\n45600\n45700\n45800\n45900\nEpoch: 28 \tTraining Loss: 1.153837 \tValidation Loss: 0.979926\n46000\n46100\n46200\n46300\n46400\n46500\n46600\n46700\n46800\n46900\n47000\n47100\n47200\n47300\n47400\n47500\nEpoch: 29 \tTraining Loss: 1.130754 \tValidation Loss: 0.646623\nValidation loss decreased (0.674529 --> 0.646623).  Saving model ...\n47600\n47700\n47800\n47900\n48000\n48100\n48200\n48300\n48400\n48500\n48600\n48700\n48800\n48900\n49000\n49100\n49200\nEpoch: 30 \tTraining Loss: 1.086662 \tValidation Loss: 0.606521\nValidation loss decreased (0.646623 --> 0.606521).  Saving model ...\n49300\n49400\n49500\n49600\n49700\n49800\n49900\n50000\n50100\n50200\n50300\n50400\n50500\n50600\n50700\n50800\nEpoch: 31 \tTraining Loss: 1.042548 \tValidation Loss: 0.735312\n50900\n51000\n51100\n51200\n51300\n51400\n51500\n51600\n51700\n51800\n51900\n52000\n52100\n52200\n52300\n52400\n52500\nEpoch: 32 \tTraining Loss: 1.017084 \tValidation Loss: 0.736268\n52600\n52700\n52800\n52900\n53000\n53100\n53200\n53300\n53400\n53500\n53600\n53700\n53800\n53900\n54000\n54100\nEpoch: 33 \tTraining Loss: 0.998482 \tValidation Loss: 0.548096\nValidation loss decreased (0.606521 --> 0.548096).  Saving model ...\n54200\n54300\n54400\n54500\n54600\n54700\n54800\n54900\n55000\n55100\n55200\n55300\n55400\n55500\n55600\n55700\nEpoch: 34 \tTraining Loss: 0.984408 \tValidation Loss: 0.605808\n55800\n55900\n56000\n56100\n56200\n56300\n56400\n56500\n56600\n","output_type":"stream"}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'model_bird_species_4.pt')"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing \n","We have trained and saved 2 more models -  \n","1. model_bird_species_3 - is the model where we are getting the min Validation loss ( but high training loss) on new training data.  \n","2. model_bird_species_4 - is the model where are fitting the data efficiently to the training data but have high loss in validation data. (on new training data)  \n","\n","Choose 1 saved model in the below code to see the what accuracy we are getting on each model."],"metadata":{}},{"cell_type":"code","source":["# Choose one of the four available saved models- \n","#model.load_state_dict(torch.load('../input/saved-model-bird-species/model_bird_species.pt'))\n","# model.load_state_dict(torch.load('../input/saved-model-bird-species/model_bird_species_2.pt'))\n","model.load_state_dict(torch.load('../input/saved-model-bird-species/model_bird_species_3.pt'))\n","# model.load_state_dict(torch.load('../input/saved-model-bird-species/model_bird_species_4.pt'))\n","\n","test_loss = 0.0\n","class_correct = list(0. for i in range(275))\n","class_total = list(0. for i in range(275))\n","\n","model.eval()\n","# iterate over test data\n","for data, target in test_dataloader:\n","    \n","    if train_on_gpu:\n","        data, target = data.cuda(), target.cuda()\n","    \n","    output = model(data)    \n","    loss = criterion(output, target)\n","    \n","    test_loss += loss.item()*data.size(0)\n","    \n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    # calculate test accuracy for each object class\n","    batchsize=data.size(0)\n","    for i in range(batchsize):\n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# average test loss\n","test_loss = test_loss/len(test_dataloader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(275):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of class %5s: %2d%% (%2d/%2d)' % (\n","            i, 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))"],"metadata":{"execution":{"iopub.status.busy":"2021-07-18T03:48:32.696669Z","iopub.execute_input":"2021-07-18T03:48:32.696989Z","iopub.status.idle":"2021-07-18T03:48:53.594816Z","shell.execute_reply.started":"2021-07-18T03:48:32.696957Z","shell.execute_reply":"2021-07-18T03:48:53.594052Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Test Loss: 0.587593\n\nTest Accuracy of class     0: 100% ( 5/ 5)\nTest Accuracy of class     1: 100% ( 5/ 5)\nTest Accuracy of class     2: 100% ( 5/ 5)\nTest Accuracy of class     3: 80% ( 4/ 5)\nTest Accuracy of class     4: 80% ( 4/ 5)\nTest Accuracy of class     5: 80% ( 4/ 5)\nTest Accuracy of class     6: 80% ( 4/ 5)\nTest Accuracy of class     7: 100% ( 5/ 5)\nTest Accuracy of class     8: 100% ( 5/ 5)\nTest Accuracy of class     9: 100% ( 5/ 5)\nTest Accuracy of class    10: 100% ( 5/ 5)\nTest Accuracy of class    11: 100% ( 5/ 5)\nTest Accuracy of class    12: 80% ( 4/ 5)\nTest Accuracy of class    13: 100% ( 5/ 5)\nTest Accuracy of class    14: 100% ( 5/ 5)\nTest Accuracy of class    15: 100% ( 5/ 5)\nTest Accuracy of class    16: 100% ( 5/ 5)\nTest Accuracy of class    17: 100% ( 5/ 5)\nTest Accuracy of class    18: 100% ( 5/ 5)\nTest Accuracy of class    19: 100% ( 5/ 5)\nTest Accuracy of class    20: 100% ( 5/ 5)\nTest Accuracy of class    21:  0% ( 0/ 5)\nTest Accuracy of class    22: 100% ( 5/ 5)\nTest Accuracy of class    23: 100% ( 5/ 5)\nTest Accuracy of class    24: 100% ( 5/ 5)\nTest Accuracy of class    25: 100% ( 5/ 5)\nTest Accuracy of class    26: 100% ( 5/ 5)\nTest Accuracy of class    27: 100% ( 5/ 5)\nTest Accuracy of class    28: 100% ( 5/ 5)\nTest Accuracy of class    29: 80% ( 4/ 5)\nTest Accuracy of class    30: 100% ( 5/ 5)\nTest Accuracy of class    31: 100% ( 5/ 5)\nTest Accuracy of class    32: 80% ( 4/ 5)\nTest Accuracy of class    33: 100% ( 5/ 5)\nTest Accuracy of class    34: 100% ( 5/ 5)\nTest Accuracy of class    35: 100% ( 5/ 5)\nTest Accuracy of class    36: 100% ( 5/ 5)\nTest Accuracy of class    37: 100% ( 5/ 5)\nTest Accuracy of class    38: 60% ( 3/ 5)\nTest Accuracy of class    39: 100% ( 5/ 5)\nTest Accuracy of class    40: 100% ( 5/ 5)\nTest Accuracy of class    41: 100% ( 5/ 5)\nTest Accuracy of class    42: 80% ( 4/ 5)\nTest Accuracy of class    43: 80% ( 4/ 5)\nTest Accuracy of class    44: 100% ( 5/ 5)\nTest Accuracy of class    45: 100% ( 5/ 5)\nTest Accuracy of class    46: 80% ( 4/ 5)\nTest Accuracy of class    47: 100% ( 5/ 5)\nTest Accuracy of class    48: 100% ( 5/ 5)\nTest Accuracy of class    49: 100% ( 5/ 5)\nTest Accuracy of class    50: 80% ( 4/ 5)\nTest Accuracy of class    51: 100% ( 5/ 5)\nTest Accuracy of class    52: 100% ( 5/ 5)\nTest Accuracy of class    53: 100% ( 5/ 5)\nTest Accuracy of class    54: 100% ( 5/ 5)\nTest Accuracy of class    55: 80% ( 4/ 5)\nTest Accuracy of class    56: 80% ( 4/ 5)\nTest Accuracy of class    57: 100% ( 5/ 5)\nTest Accuracy of class    58: 100% ( 5/ 5)\nTest Accuracy of class    59: 80% ( 4/ 5)\nTest Accuracy of class    60: 100% ( 5/ 5)\nTest Accuracy of class    61: 100% ( 5/ 5)\nTest Accuracy of class    62:  0% ( 0/ 5)\nTest Accuracy of class    63: 100% ( 5/ 5)\nTest Accuracy of class    64: 100% ( 5/ 5)\nTest Accuracy of class    65: 80% ( 4/ 5)\nTest Accuracy of class    66: 100% ( 5/ 5)\nTest Accuracy of class    67: 100% ( 5/ 5)\nTest Accuracy of class    68: 80% ( 4/ 5)\nTest Accuracy of class    69: 100% ( 5/ 5)\nTest Accuracy of class    70: 80% ( 4/ 5)\nTest Accuracy of class    71: 100% ( 5/ 5)\nTest Accuracy of class    72: 100% ( 5/ 5)\nTest Accuracy of class    73: 100% ( 5/ 5)\nTest Accuracy of class    74: 100% ( 5/ 5)\nTest Accuracy of class    75: 100% ( 5/ 5)\nTest Accuracy of class    76: 100% ( 5/ 5)\nTest Accuracy of class    77: 60% ( 3/ 5)\nTest Accuracy of class    78: 80% ( 4/ 5)\nTest Accuracy of class    79: 80% ( 4/ 5)\nTest Accuracy of class    80: 80% ( 4/ 5)\nTest Accuracy of class    81: 100% ( 5/ 5)\nTest Accuracy of class    82: 100% ( 5/ 5)\nTest Accuracy of class    83: 60% ( 3/ 5)\nTest Accuracy of class    84: 100% ( 5/ 5)\nTest Accuracy of class    85: 100% ( 5/ 5)\nTest Accuracy of class    86:  0% ( 0/ 5)\nTest Accuracy of class    87: 100% ( 5/ 5)\nTest Accuracy of class    88: 100% ( 5/ 5)\nTest Accuracy of class    89: 80% ( 4/ 5)\nTest Accuracy of class    90: 100% ( 5/ 5)\nTest Accuracy of class    91: 80% ( 4/ 5)\nTest Accuracy of class    92: 100% ( 5/ 5)\nTest Accuracy of class    93: 100% ( 5/ 5)\nTest Accuracy of class    94: 100% ( 5/ 5)\nTest Accuracy of class    95: 100% ( 5/ 5)\nTest Accuracy of class    96:  0% ( 0/ 5)\nTest Accuracy of class    97:  0% ( 0/ 5)\nTest Accuracy of class    98: 100% ( 5/ 5)\nTest Accuracy of class    99: 80% ( 4/ 5)\nTest Accuracy of class   100: 100% ( 5/ 5)\nTest Accuracy of class   101: 80% ( 4/ 5)\nTest Accuracy of class   102: 100% ( 5/ 5)\nTest Accuracy of class   103: 100% ( 5/ 5)\nTest Accuracy of class   104: 100% ( 5/ 5)\nTest Accuracy of class   105: 100% ( 5/ 5)\nTest Accuracy of class   106: 100% ( 5/ 5)\nTest Accuracy of class   107: 100% ( 5/ 5)\nTest Accuracy of class   108: 100% ( 5/ 5)\nTest Accuracy of class   109: 80% ( 4/ 5)\nTest Accuracy of class   110: 100% ( 5/ 5)\nTest Accuracy of class   111:  0% ( 0/ 5)\nTest Accuracy of class   112: 100% ( 5/ 5)\nTest Accuracy of class   113: 100% ( 5/ 5)\nTest Accuracy of class   114: 100% ( 5/ 5)\nTest Accuracy of class   115: 100% ( 5/ 5)\nTest Accuracy of class   116: 80% ( 4/ 5)\nTest Accuracy of class   117: 80% ( 4/ 5)\nTest Accuracy of class   118: 100% ( 5/ 5)\nTest Accuracy of class   119: 100% ( 5/ 5)\nTest Accuracy of class   120: 80% ( 4/ 5)\nTest Accuracy of class   121: 60% ( 3/ 5)\nTest Accuracy of class   122: 100% ( 5/ 5)\nTest Accuracy of class   123: 100% ( 5/ 5)\nTest Accuracy of class   124: 60% ( 3/ 5)\nTest Accuracy of class   125: 100% ( 5/ 5)\nTest Accuracy of class   126: 100% ( 5/ 5)\nTest Accuracy of class   127: 100% ( 5/ 5)\nTest Accuracy of class   128: 100% ( 5/ 5)\nTest Accuracy of class   129:  0% ( 0/ 5)\nTest Accuracy of class   130: 100% ( 5/ 5)\nTest Accuracy of class   131: 100% ( 5/ 5)\nTest Accuracy of class   132: 100% ( 5/ 5)\nTest Accuracy of class   133:  0% ( 0/ 5)\nTest Accuracy of class   134: 100% ( 5/ 5)\nTest Accuracy of class   135: 100% ( 5/ 5)\nTest Accuracy of class   136: 80% ( 4/ 5)\nTest Accuracy of class   137: 100% ( 5/ 5)\nTest Accuracy of class   138: 100% ( 5/ 5)\nTest Accuracy of class   139: 80% ( 4/ 5)\nTest Accuracy of class   140: 100% ( 5/ 5)\nTest Accuracy of class   141: 100% ( 5/ 5)\nTest Accuracy of class   142: 100% ( 5/ 5)\nTest Accuracy of class   143: 100% ( 5/ 5)\nTest Accuracy of class   144: 100% ( 5/ 5)\nTest Accuracy of class   145: 100% ( 5/ 5)\nTest Accuracy of class   146: 100% ( 5/ 5)\nTest Accuracy of class   147: 100% ( 5/ 5)\nTest Accuracy of class   148: 60% ( 3/ 5)\nTest Accuracy of class   149: 100% ( 5/ 5)\nTest Accuracy of class   150: 100% ( 5/ 5)\nTest Accuracy of class   151: 100% ( 5/ 5)\nTest Accuracy of class   152: 80% ( 4/ 5)\nTest Accuracy of class   153: 100% ( 5/ 5)\nTest Accuracy of class   154: 80% ( 4/ 5)\nTest Accuracy of class   155: 80% ( 4/ 5)\nTest Accuracy of class   156: 100% ( 5/ 5)\nTest Accuracy of class   157: 80% ( 4/ 5)\nTest Accuracy of class   158:  0% ( 0/ 5)\nTest Accuracy of class   159: 100% ( 5/ 5)\nTest Accuracy of class   160: 60% ( 3/ 5)\nTest Accuracy of class   161: 100% ( 5/ 5)\nTest Accuracy of class   162: 100% ( 5/ 5)\nTest Accuracy of class   163: 60% ( 3/ 5)\nTest Accuracy of class   164: 100% ( 5/ 5)\nTest Accuracy of class   165: 100% ( 5/ 5)\nTest Accuracy of class   166: 100% ( 5/ 5)\nTest Accuracy of class   167: 80% ( 4/ 5)\nTest Accuracy of class   168: 20% ( 1/ 5)\nTest Accuracy of class   169: 80% ( 4/ 5)\nTest Accuracy of class   170: 100% ( 5/ 5)\nTest Accuracy of class   171: 60% ( 3/ 5)\nTest Accuracy of class   172: 100% ( 5/ 5)\nTest Accuracy of class   173: 100% ( 5/ 5)\nTest Accuracy of class   174: 100% ( 5/ 5)\nTest Accuracy of class   175: 100% ( 5/ 5)\nTest Accuracy of class   176: 100% ( 5/ 5)\nTest Accuracy of class   177: 60% ( 3/ 5)\nTest Accuracy of class   178: 100% ( 5/ 5)\nTest Accuracy of class   179: 80% ( 4/ 5)\nTest Accuracy of class   180: 100% ( 5/ 5)\nTest Accuracy of class   181: 100% ( 5/ 5)\nTest Accuracy of class   182: 100% ( 5/ 5)\nTest Accuracy of class   183: 100% ( 5/ 5)\nTest Accuracy of class   184: 100% ( 5/ 5)\nTest Accuracy of class   185: 100% ( 5/ 5)\nTest Accuracy of class   186: 100% ( 5/ 5)\nTest Accuracy of class   187: 100% ( 5/ 5)\nTest Accuracy of class   188: 100% ( 5/ 5)\nTest Accuracy of class   189: 20% ( 1/ 5)\nTest Accuracy of class   190: 100% ( 5/ 5)\nTest Accuracy of class   191: 100% ( 5/ 5)\nTest Accuracy of class   192: 100% ( 5/ 5)\nTest Accuracy of class   193: 100% ( 5/ 5)\nTest Accuracy of class   194: 100% ( 5/ 5)\nTest Accuracy of class   195: 100% ( 5/ 5)\nTest Accuracy of class   196: 80% ( 4/ 5)\nTest Accuracy of class   197: 80% ( 4/ 5)\nTest Accuracy of class   198: 100% ( 5/ 5)\nTest Accuracy of class   199: 100% ( 5/ 5)\nTest Accuracy of class   200: 100% ( 5/ 5)\nTest Accuracy of class   201: 100% ( 5/ 5)\nTest Accuracy of class   202: 100% ( 5/ 5)\nTest Accuracy of class   203: 100% ( 5/ 5)\nTest Accuracy of class   204: 100% ( 5/ 5)\nTest Accuracy of class   205: 100% ( 5/ 5)\nTest Accuracy of class   206: 100% ( 5/ 5)\nTest Accuracy of class   207: 100% ( 5/ 5)\nTest Accuracy of class   208: 100% ( 5/ 5)\nTest Accuracy of class   209: 100% ( 5/ 5)\nTest Accuracy of class   210: 100% ( 5/ 5)\nTest Accuracy of class   211: 100% ( 5/ 5)\nTest Accuracy of class   212: 100% ( 5/ 5)\nTest Accuracy of class   213: 100% ( 5/ 5)\nTest Accuracy of class   214: 100% ( 5/ 5)\nTest Accuracy of class   215: 80% ( 4/ 5)\nTest Accuracy of class   216: 100% ( 5/ 5)\nTest Accuracy of class   217: 100% ( 5/ 5)\nTest Accuracy of class   218: 100% ( 5/ 5)\nTest Accuracy of class   219: 80% ( 4/ 5)\nTest Accuracy of class   220: 100% ( 5/ 5)\nTest Accuracy of class   221: 100% ( 5/ 5)\nTest Accuracy of class   222: 80% ( 4/ 5)\nTest Accuracy of class   223: 100% ( 5/ 5)\nTest Accuracy of class   224: 100% ( 5/ 5)\nTest Accuracy of class   225: 60% ( 3/ 5)\nTest Accuracy of class   226:  0% ( 0/ 5)\nTest Accuracy of class   227: 100% ( 5/ 5)\nTest Accuracy of class   228: 80% ( 4/ 5)\nTest Accuracy of class   229: 100% ( 5/ 5)\nTest Accuracy of class   230: 60% ( 3/ 5)\nTest Accuracy of class   231: 80% ( 4/ 5)\nTest Accuracy of class   232: 80% ( 4/ 5)\nTest Accuracy of class   233: 60% ( 3/ 5)\nTest Accuracy of class   234: 80% ( 4/ 5)\nTest Accuracy of class   235: 100% ( 5/ 5)\nTest Accuracy of class   236: 100% ( 5/ 5)\nTest Accuracy of class   237: 100% ( 5/ 5)\nTest Accuracy of class   238: 100% ( 5/ 5)\nTest Accuracy of class   239:  0% ( 0/ 5)\nTest Accuracy of class   240: 100% ( 5/ 5)\nTest Accuracy of class   241: 100% ( 5/ 5)\nTest Accuracy of class   242:  0% ( 0/ 5)\nTest Accuracy of class   243: 100% ( 5/ 5)\nTest Accuracy of class   244: 100% ( 5/ 5)\nTest Accuracy of class   245:  0% ( 0/ 5)\nTest Accuracy of class   246: 100% ( 5/ 5)\nTest Accuracy of class   247: 100% ( 5/ 5)\nTest Accuracy of class   248: 100% ( 5/ 5)\nTest Accuracy of class   249: 80% ( 4/ 5)\nTest Accuracy of class   250: 100% ( 5/ 5)\nTest Accuracy of class   251: 60% ( 3/ 5)\nTest Accuracy of class   252: 100% ( 5/ 5)\nTest Accuracy of class   253: 60% ( 3/ 5)\nTest Accuracy of class   254: 100% ( 5/ 5)\nTest Accuracy of class   255: 100% ( 5/ 5)\nTest Accuracy of class   256: 100% ( 5/ 5)\nTest Accuracy of class   257: 100% ( 5/ 5)\nTest Accuracy of class   258:  0% ( 0/ 5)\nTest Accuracy of class   259: 100% ( 5/ 5)\nTest Accuracy of class   260: 100% ( 5/ 5)\nTest Accuracy of class   261: 100% ( 5/ 5)\nTest Accuracy of class   262: 80% ( 4/ 5)\nTest Accuracy of class   263: 100% ( 5/ 5)\nTest Accuracy of class   264: 20% ( 1/ 5)\nTest Accuracy of class   265: 100% ( 5/ 5)\nTest Accuracy of class   266: 100% ( 5/ 5)\nTest Accuracy of class   267: 100% ( 5/ 5)\nTest Accuracy of class   268: 80% ( 4/ 5)\nTest Accuracy of class   269: 100% ( 5/ 5)\nTest Accuracy of class   270:  0% ( 0/ 5)\nTest Accuracy of class   271: 100% ( 5/ 5)\nTest Accuracy of class   272: 100% ( 5/ 5)\nTest Accuracy of class   273: 100% ( 5/ 5)\nTest Accuracy of class   274: 100% ( 5/ 5)\n\nTest Accuracy (Overall): 87% (1209/1375)\n","output_type":"stream"}]},{"source":["## NICEEEE...\n","We got a nice accuracy for our model\n","\n","## NOTE:  \n"," As mentioned in the analysis, I tried the Adam model (for lr 0.00005) and as pridicted, it was very slow -   \n"," These Were its epoch results - \n","\n","Epoch: 1 \tTraining Loss: 5.393564 \tValidation Loss: 5.140303\n","Validation loss decreased (inf --> 5.140303).  Saving model ...\n","\n","Epoch: 2 \tTraining Loss: 5.118051 \tValidation Loss: 4.848226\n","Validation loss decreased (5.140303 --> 4.848226).  Saving model ...\n","\n","Epoch: 3 \tTraining Loss: 4.935542 \tValidation Loss: 4.700039\n","Validation loss decreased (4.848226 --> 4.700039).  Saving model ...\n","\n","Epoch: 4 \tTraining Loss: 4.788703 \tValidation Loss: 4.513692\n","Validation loss decreased (4.700039 --> 4.513692).  Saving model ...\n","\n","Epoch: 5 \tTraining Loss: 4.682527 \tValidation Loss: 4.396898\n","Validation loss decreased (4.513692 --> 4.396898).  Saving model ...\n","\n","Epoch: 6 \tTraining Loss: 4.599165 \tValidation Loss: 4.295371\n","Validation loss decreased (4.396898 --> 4.295371).  Saving model ...\n","\n","Epoch: 7 \tTraining Loss: 4.530815 \tValidation Loss: 4.229605\n","Validation loss decreased (4.295371 --> 4.229605).  Saving model ...\n","\n","Epoch: 8 \tTraining Loss: 4.476235 \tValidation Loss: 4.201569\n","Validation loss decreased (4.229605 --> 4.201569).  Saving model ...\n","\n","Epoch: 9 \tTraining Loss: 4.422969 \tValidation Loss: 4.143393\n","Validation loss decreased (4.201569 --> 4.143393).  Saving model ...\n","\n","Epoch: 10 \tTraining Loss: 4.380741 \tValidation Loss: 4.109100\n","Validation loss decreased (4.143393 --> 4.109100).  Saving model ...\n","\n","## Therefore, I discarded the Adam model and trained the data with SGD which gave us the accuracy of about 90% !!"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}