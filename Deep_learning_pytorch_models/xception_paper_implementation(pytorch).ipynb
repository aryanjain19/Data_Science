{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c0f70214c0dd213f07f54ee5d6e0ea644bdbba35113c9bfe8aaa0d1db03ad5dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "train_on_gpu=False\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.Resize([299, 299]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([299, 299]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception_implementation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Xception_implementation,self).__init__()\n",
    "\n",
    "        self.c1=nn.Conv2d(3,32,3,stride=2)\n",
    "        self.bn1=nn.BatchNorm2d(32)\n",
    "\n",
    "        self.c2=nn.Conv2d(32,64,3)\n",
    "        self.bn2=nn.BatchNorm2d(64)\n",
    "\n",
    "        self.c3=nn.Conv2d(64,128,1,stride=2)\n",
    "        self.bn3=nn.BatchNorm2d(128)\n",
    "\n",
    "        self.c4=nn.Conv2d(128,256,1,stride=2)\n",
    "        self.bn4=nn.BatchNorm2d(256)\n",
    "\n",
    "        self.c5=nn.Conv2d(256,728,1,stride=2)\n",
    "        self.bn5=nn.BatchNorm2d(728)\n",
    "\n",
    "        self.c6=nn.Conv2d(728,728,1,stride=2)\n",
    "        #self.bn5=nn.BatchNorm2d(728)\n",
    "\n",
    "        self.c7=nn.Conv2d(728,1024,1,stride=2)\n",
    "        self.bn6=nn.BatchNorm2d(1024)\n",
    "\n",
    "\n",
    "        self.sc1a=nn.Conv2d(64,64,3,groups=64,padding=1,bias=False)\n",
    "        self.sc1b=nn.Conv2d(64,128,1,bias=False)\n",
    "        #self.bn3=nn.BatchNorm2d(128)\n",
    "\n",
    "        self.sc2a=nn.Conv2d(128,128,3,groups=128,padding=1,bias=False)\n",
    "        self.sc2b=nn.Conv2d(128,128,1,bias=False)\n",
    "        #self.bn3=nn.BatchNorm2d(128)\n",
    "\n",
    "        self.sc3a=nn.Conv2d(128,128,3,groups=128,padding=1,bias=False)\n",
    "        self.sc3b=nn.Conv2d(128,256,1,bias=False)\n",
    "        #self.bn4=nn.BatchNorm2d(256)\n",
    "\n",
    "        self.sc4a=nn.Conv2d(256,256,3,groups=256,padding=1,bias=False)\n",
    "        self.sc4b=nn.Conv2d(256,256,1,bias=False)\n",
    "        #self.bn4=nn.BatchNorm2d(256)\n",
    "\n",
    "        self.sc5a=nn.Conv2d(256,256,3,groups=256,padding=1,bias=False)\n",
    "        self.sc5b=nn.Conv2d(256,728,1,bias=False)\n",
    "        #self.bn5=nn.BatchNorm2d(728)\n",
    "\n",
    "        self.sc6a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n",
    "        self.sc6b=nn.Conv2d(728,728,1,bias=False)\n",
    "        #self.bn5=nn.BatchNorm2d(728)\n",
    "\n",
    "        self.sc7a=nn.Conv2d(728,728,3,groups=728,padding=1,bias=False)\n",
    "        self.sc7b=nn.Conv2d(728,1024,1,bias=False)\n",
    "        #self.bn6=nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.sc8a=nn.Conv2d(1024,1024,3,groups=1024,padding=1,bias=False)\n",
    "        self.sc8b=nn.Conv2d(1024,1536,1,bias=False)\n",
    "        self.bn7=nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.sc9a=nn.Conv2d(1536,1536,3,groups=1536,padding=1,bias=False)\n",
    "        self.sc9b=nn.Conv2d(1536,2048,1,bias=False)\n",
    "        self.bn8=nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc1=nn.Linear(2048,512)\n",
    "        self.ol=nn.Linear(512,10)\n",
    "\n",
    "\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=(3,3),stride=2,padding=1)\n",
    "        self.avgpool=nn.AvgPool2d(kernel_size=(10,10))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.c1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=F.relu(x)\n",
    "\n",
    "        x=self.c2(x)\n",
    "        x=self.bn2(x)\n",
    "        x=F.relu(x)\n",
    "        output=self.c3(x)\n",
    "        output=self.bn3(output)\n",
    "\n",
    "        x=self.sc1b(self.sc1a(x))\n",
    "        x=self.bn3(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.sc2b(self.sc2a(x))\n",
    "        x=self.bn3(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=x+output\n",
    "        output=self.c4(output)\n",
    "        output=self.bn4(output)\n",
    "\n",
    "        x=self.sc3b(self.sc3a(F.relu(x)))\n",
    "        x=self.bn4(x)\n",
    "        x=self.sc4b(self.sc4a(F.relu(x)))\n",
    "        x=self.bn4(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=x+output\n",
    "        output=self.c5(output)\n",
    "        output=self.bn5(output)\n",
    "        \n",
    "        x=self.sc5b(self.sc5a(F.relu(x)))\n",
    "        x=self.bn5(x)\n",
    "        x=self.sc6b(self.sc6a(F.relu(x)))\n",
    "        x=self.bn5(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=x+output\n",
    "\n",
    "        output=x\n",
    "        for _ in range(8):\n",
    "            x=self.sc6b(self.sc6a(F.relu(x)))\n",
    "            x=self.bn5(x)\n",
    "            x=self.sc6b(self.sc6a(F.relu(x)))\n",
    "            x=self.bn5(x)\n",
    "            x=self.sc6b(self.sc6a(F.relu(x)))\n",
    "            x=self.bn5(x)\n",
    "            x=x+output\n",
    "        \n",
    "        output=self.c7(x)\n",
    "        output=self.bn6(output)\n",
    "\n",
    "        x=self.sc6b(self.sc6a(F.relu(x)))\n",
    "        x=self.bn5(x)\n",
    "        x=self.sc7b(self.sc7a(F.relu(x)))\n",
    "        x=self.bn6(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=x+output\n",
    "\n",
    "        x=self.sc8b(self.sc8a(x))\n",
    "        x=self.bn7(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.sc9b(self.sc9a(x))\n",
    "        x=self.bn8(x)\n",
    "        x=F.relu(x)\n",
    "\n",
    "        x=self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.ol(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Xception_implementation()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1 \n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "batch_number=0\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        batch_number+=1\n",
    "        print(batch_number)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    " \n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}